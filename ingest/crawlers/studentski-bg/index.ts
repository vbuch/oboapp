#!/usr/bin/env node

import dotenv from "dotenv";
import { resolve } from "node:path";
import { Browser } from "playwright";
import type { Firestore } from "firebase-admin/firestore";
import { SourceDocument, PostLink } from "./types";
import { launchBrowser } from "../shared/browser";
import { isUrlProcessed, saveSourceDocument } from "../shared/firestore";
import { delay } from "@/lib/delay";
import { extractPostLinks, extractPostDetails } from "./extractors";
import { buildWebPageSourceDocument } from "../shared/webpage-crawlers";

dotenv.config({ path: resolve(process.cwd(), ".env.local") });

const INDEX_URL =
  "https://studentski.bg/category/%d0%b3%d1%80%d0%b0%d1%84%d0%b8%d1%86%d0%b8/";
const SOURCE_TYPE = "studentski-bg";
const DELAY_BETWEEN_REQUESTS = 2000; // 2 seconds

/**
 * Main crawler function for studentski.bg
 */
export async function crawl(): Promise<void> {
  console.log("üöÄ Starting studentski-bg crawler...\n");
  console.log(`üìç Index URL: ${INDEX_URL}`);
  console.log(`üóÑÔ∏è  Source type: ${SOURCE_TYPE}\n`);

  const { adminDb } = await import("@/lib/firebase-admin");
  let browser: Browser | null = null;

  try {
    console.log("üåê Launching browser...");
    browser = await launchBrowser();

    const page = await browser.newPage();
    console.log(`üì• Fetching index page: ${INDEX_URL}`);
    await page.goto(INDEX_URL, { waitUntil: "networkidle" });

    const postLinks = await extractPostLinks(page);
    await page.close();

    if (postLinks.length === 0) {
      console.warn("‚ö†Ô∏è No posts found on index page");
      return;
    }

    console.log(`\nüìä Total posts to process: ${postLinks.length}\n`);

    let processedCount = 0;
    let skippedCount = 0;

    for (const postLink of postLinks) {
      try {
        const wasProcessed = await isUrlProcessed(postLink.url, adminDb);
        if (wasProcessed) {
          skippedCount++;
          console.log(
            `‚è≠Ô∏è  Skipped (already processed): ${postLink.title.substring(
              0,
              60
            )}...`
          );
        } else {
          await processPost(browser, postLink, adminDb);
          processedCount++;
        }
      } catch (error) {
        console.error(`‚ùå Error processing post: ${postLink.url}`, error);
      }
    }

    console.log("\n" + "=".repeat(60));
    console.log("‚úÖ Crawling completed successfully!");
    console.log(`üìä Total posts found: ${postLinks.length}`);
    console.log(`‚úÖ Newly processed: ${processedCount}`);
    console.log(`‚è≠Ô∏è  Skipped (already exists): ${skippedCount}`);
    console.log("=".repeat(60) + "\n");
  } catch (error) {
    console.error("\n" + "=".repeat(60));
    console.error("‚ùå Crawling failed with error:");
    console.error(error);
    console.error("=".repeat(60) + "\n");
    throw error;
  } finally {
    if (browser) {
      await browser.close();
      console.log("üîí Browser closed");
    }
  }
}

/**
 * Process a single post
 */
async function processPost(
  browser: Browser,
  postLink: PostLink,
  adminDb: Firestore
): Promise<void> {
  const { url, title } = postLink;

  console.log(`\nüîç Processing: ${title.substring(0, 60)}...`);

  try {
    const alreadyProcessed = await isUrlProcessed(url, adminDb);
    if (alreadyProcessed) {
      console.log(`‚è≠Ô∏è  Skipped (already processed): ${url}`);
      return;
    }
  } catch (error) {
    console.error(`‚ùå Error checking if URL is processed: ${url}`, error);
    throw error;
  }

  const page = await browser.newPage();

  try {
    console.log(`üì• Fetching: ${url}`);
    await page.goto(url, { waitUntil: "networkidle" });

    const details = await extractPostDetails(page);

    const postDetails = buildWebPageSourceDocument(
      url,
      details.title,
      details.dateText,
      details.contentHtml,
      SOURCE_TYPE
    ) as Omit<SourceDocument, "crawledAt">;

    const sourceDoc: SourceDocument = {
      ...postDetails,
      crawledAt: new Date(),
    };

    await saveSourceDocument(sourceDoc, adminDb);

    console.log(`‚úÖ Successfully processed: ${title.substring(0, 60)}...`);
  } catch (error) {
    console.error(`‚ùå Error processing post: ${url}`, error);
    throw error;
  } finally {
    await page.close();
  }

  await delay(DELAY_BETWEEN_REQUESTS);
}

if (require.main === module) {
  crawl().catch((error) => {
    console.error("Fatal error:", error);
    process.exit(1);
  });
}
