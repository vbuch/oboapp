import { parseBulgarianDate } from "./date-utils";
import { createTurndownService } from "./markdown";
import { Browser, Page } from "playwright";
import type { Firestore } from "firebase-admin/firestore";
import { PostLink } from "./types";
import { launchBrowser } from "./browser";
import { isUrlProcessed } from "./firestore";

const turndownService = createTurndownService();

/**
 * Build a SourceDocument from webpage content (HTML to Markdown conversion)
 * Used by WordPress-style crawlers like rayon-oborishte-bg and sofia-bg
 */
export function buildWebPageSourceDocument(
  url: string,
  title: string,
  dateText: string,
  contentHtml: string,
  sourceType: string,
  customDateParser?: (dateText: string) => string
): {
  url: string;
  title: string;
  datePublished: string;
  message: string;
  sourceType: string;
} {
  if (!title) {
    throw new Error(`Failed to extract title from ${url}`);
  }

  if (!contentHtml) {
    throw new Error(`Failed to extract content from ${url}`);
  }

  // Convert HTML to Markdown
  const message = turndownService.turndown(contentHtml);

  // Parse date to ISO format (use custom parser if provided)
  const datePublished = customDateParser
    ? customDateParser(dateText)
    : parseBulgarianDate(dateText);

  return {
    url,
    title,
    datePublished,
    message,
    sourceType,
  };
}

/**
 * Crawl a WordPress-style news page
 * Generic crawler that extracts post links from an index page and processes each post
 */
export async function crawlWordpressPage(options: {
  indexUrl: string;
  sourceType: string;
  extractPostLinks: (page: Page) => Promise<PostLink[]>;
  processPost: (
    browser: Browser,
    postLink: PostLink,
    adminDb: Firestore
  ) => Promise<void>;
  delayBetweenRequests?: number;
}): Promise<void> {
  const {
    indexUrl,
    sourceType,
    extractPostLinks,
    processPost,
    delayBetweenRequests = 2000,
  } = options;

  console.log(`ğŸš€ Starting ${sourceType} crawler...\n`);
  console.log(`ğŸ“ Index URL: ${indexUrl}`);
  console.log(`ğŸ—„ï¸  Source type: ${sourceType}\n`);

  const { adminDb } = await import("@/lib/firebase-admin");

  let browser: Browser | null = null;

  try {
    console.log("ğŸŒ Launching browser...");
    browser = await launchBrowser();

    const page = await browser.newPage();
    console.log(`ğŸ“¥ Fetching index page: ${indexUrl}`);
    await page.goto(indexUrl, { waitUntil: "networkidle" });

    const postLinks = await extractPostLinks(page);
    await page.close();

    if (postLinks.length === 0) {
      console.warn("âš ï¸ No posts found on index page");
      return;
    }

    console.log(`\nğŸ“Š Total posts to process: ${postLinks.length}\n`);

    let processedCount = 0;
    let skippedCount = 0;

    for (const postLink of postLinks) {
      try {
        const wasProcessed = await isUrlProcessed(postLink.url, adminDb);

        if (wasProcessed) {
          skippedCount++;
          console.log(
            `â­ï¸  Skipped (already processed): ${postLink.title.substring(
              0,
              60
            )}...`
          );
        } else {
          await processPost(browser, postLink, adminDb);
          processedCount++;
        }
      } catch (error) {
        console.error(`âŒ Error processing post: ${postLink.url}`, error);
      }
    }

    console.log("\n" + "=".repeat(60));
    console.log("âœ… Crawling completed successfully!");
    console.log(`ğŸ“Š Total posts found: ${postLinks.length}`);
    console.log(`âœ… Newly processed: ${processedCount}`);
    console.log(`â­ï¸  Skipped (already exists): ${skippedCount}`);
    console.log("=".repeat(60) + "\n");
  } catch (error) {
    console.error("\n" + "=".repeat(60));
    console.error("âŒ Crawling failed with error:");
    console.error(error);
    console.error("=".repeat(60) + "\n");
    throw error;
  } finally {
    if (browser) {
      await browser.close();
      console.log("ğŸ”’ Browser closed");
    }
  }
}
